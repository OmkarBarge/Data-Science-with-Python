{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis)\n",
    "\n",
    "1) It is an unsupervised learning algorithm used for dimensionality reduction<br>\n",
    "2) It reduces the number of features(columns). <br>\n",
    "3) It can serve as a data cleaning or data preprocessing technique used before applying other ML algorithm.<br>\n",
    "3) So after applying PCA, we can apply Regression, Classification or even Clustering<br> \n",
    "4) Principal components so generated remove noise by reducing a large number of features to just a couple of principal components. Principal components are orthogonal projections of data onto lower-dimensional space.<br>\n",
    "5) In theory, PCA produces the same number of principal components as there are features in the training dataset. In practice, though, we do not keep all of the principal components. Each successive principal component explains the variance that is left after its preceding component, so picking just a few of the first components sufficiently approximates the original dataset without the need for additional features.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Steps\n",
    "\n",
    "#### 1) Standardization (StandardScaler)\n",
    "Standardized value (z) = (Actual Value – mean)/ Standard Deviation \n",
    "\n",
    "#### 2) Covariance Matrix Computation\n",
    "a) For the standardized dataset, PCA computes the Covariance matrix in the second step.<br>\n",
    "b) Covariance describes the direction of linear relationship between 2 variables<br>\n",
    "c) Covaraince matrix is a square matrix, the shape depends upon the number of varaibles<br>\n",
    "\n",
    "<img src=\"pca1.png\" align=\"left\" width=\"350\">\n",
    "<img src=\"pca2.png\" align=\"middle\" width=\"400\">\n",
    "\n",
    "<pre>\n",
    "cov(x,y) for 2 variables x and y \n",
    "cov(x,y) = [cov(x,x)  cov(x,y)]\n",
    "           [cov(y,x)  cov(y,y)]\n",
    "           </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigen Values and Eigen Vectors\n",
    "\n",
    "<b>Eigen Vector</b><br>\n",
    "An eigenvector of for a square matrix A, is a nonzero vector v in\n",
    "such that Av = λv, holds True for some scalar λ.\n",
    "\n",
    "<b>Eigen Value</b><br>\n",
    "An eigenvalue of for a square matrix A, is a scaler λ such that Av = λv has non-trivial solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components\n",
    "\n",
    "a)\tEigenvectors and eigenvalues are the linear algebra concepts that PCA computes from the covariance matrix in order to determine the principal components of the data.<br>\n",
    "b)\tPrincipal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. So, the idea is 20-dimensional data gives you 20 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on.<br>\n",
    "c)\tOrganizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables\n",
    "\n",
    "#### How PCA constructs Pricncipal Components\n",
    "a)\tThere are as many principal components as there are variables in the data, principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set.<br>\n",
    "b)\tEvery eigenvalue has an eigenvector. And their number is equal to the number of dimensions of the data. For example, for a 3-dimensional data set, there are 3 variables, therefore there are 3 eigenvalues with 3 corresponding eigenvectors.<br>\n",
    "c)\t<b>The eigenvectors of the Covariance matrix are actually the directions of the axes where there is the most variance(most information) and that we call Principal Components. And eigenvalues are simply the coefficients attached to eigenvectors, which give the amount of variance carried in each Principal Component.</b><br>\n",
    "\n",
    "d)\t<b>By ranking your eigenvectors in order of their eigenvalues, highest to lowest(descending order), you get the principal components in order of significance.</b><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Feature Vector\n",
    "a) Computing the eigenvectors and ordering them by their eigenvalues in descending order, allow us to find the principal components in order of significance. In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and with the remaining ones form a matrix of vectors that we call Feature vector.<br>\n",
    "b) So, the feature vector is simply a matrix that has as columns the eigenvectors of the components that we decide to keep. This makes it the first step towards dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Recast the Data Along the Principal Components Axes\n",
    "\n",
    "a) In the last step, the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). <br>\n",
    "<b>b) FinalDataSet = StandardizedOriginalDataSet * FeatureVector^T</b><br>\n",
    "#### OR\n",
    "<b>b) FinalDataSet = np.dot(StandardizedOriginalDataSet,FeatureVector^T)</b>\n",
    "<br>\n",
    "where <br>\n",
    "T = transpose<br>\n",
    "Finaldataset = Resultant Principal Components after Dimensionalty Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of PCA Steps\n",
    "1) Standardize the datset<br>\n",
    "2) Compute Covarince matrix for the standardized data<br>\n",
    "3) Compute eigen value and eigen vector for the covaraince matrix. Arrange the eigen vectors(direction of axes where we have maximum variance) in decreasing order of eigen values(variance).<br>\n",
    "4) Select a threshold for the sum of varaince required(usually set to 75% or more).\n",
    "Based on this threshold, select the feature Vectors(the number of eigen vectors chosen based on sum of eigen value(sum of variance, which is more than or equal to the threshold(75%)). <br>\n",
    "5) Reorient the eigen vectors into the Principal components<br>\n",
    "Principal components  = np.dot(Standardized dataset, Feature_vector.T)<br>\n",
    "where T = Transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Eigen Values and Eigen Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# numpy is aliased as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0]\n",
      " [0 4 5]\n",
      " [0 4 3]]\n",
      "(3, 3)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[2,0,0],[0,4,5],[0,4,3]])\n",
    "print(a)\n",
    "print(a.shape) # rows=3,cols=3\n",
    "print(a.ndim)  # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen values\n",
      " [-1.  8.  2.]\n",
      "Eigen vectors\n",
      " [[ 0.          0.          1.        ]\n",
      " [ 0.70710678 -0.78086881  0.        ]\n",
      " [-0.70710678 -0.62469505  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "eig_val,eig_vec = np.linalg.eig(a)\n",
    "print('Eigen values\\n',eig_val)\n",
    "print('Eigen vectors\\n',eig_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
