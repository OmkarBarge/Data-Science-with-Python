{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing scikit-learn\n",
    "# pip install scikit-learn - In CMD line\n",
    "# !pip install scikit-learn - In Jupyter cell\n",
    "# OR\n",
    "# %pip install scikit-learn - In Jupyter cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing scikit-learn\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "* Confusion Matrix is an array used to describe the performance of a classification model in Machine Learning. <br>\n",
    "* It is a method of evaluating the performance of a classification model. The idea behind this is to count the number of times instances of class 1 are classified as class 2. For example, to find out how many times the classification model has confused the images of Dog with Cat, you use the confusion matrix.<br>\n",
    "* To calculate the confusion matrix for a given classification model, you must have a set of predicted values so that they can be compared to the set of actual values. You can use it on both test and training sets.<br>\n",
    "* In Confusion Matrix, each row represents an actual class and each column represents the predicted class.<br>\n",
    "\n",
    "\n",
    "* $True Negative:$ Model has given prediction No, and the real or actual value was also No.\n",
    "* $True Positive:$ The model has predicted yes, and the actual value was also true.\n",
    "* $False Negative:$ The model has predicted no, but the actual value was Yes, it is also called as Type-II error.\n",
    "* $False Positive:$ The model has predicted Yes, but the actual value was No. It is also called a Type-I error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 2]\n",
      " [3 2]]\n"
     ]
    }
   ],
   "source": [
    "# Importing Numpy \n",
    "import numpy as np\n",
    "# Importing confusion_matrix from sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_test = np.array([0,0,0,1,0,1,1,0,1,0,0,1])\n",
    "y_pred = np.array([0,0,0,0,1,1,0,0,1,1,0,0])\n",
    "# Showing the Confusion Matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "It is one of the performance evaluation metrics of a classification-based machine learning model. It displays your model’s precision, recall, F1 score and support. <br>\n",
    "It provides a better understanding of the overall performance of our trained model. To understand the classification report of a machine learning model, you need to know all of the metrics displayed in the report<br>\n",
    "* $Precision:$ Precision is defined as the ratio of true positives to the sum of true and false positives.\n",
    "* $Recall:$ Recall is defined as the ratio of true positives to the sum of true positives and false negatives.\n",
    "* $F1 Score:$ The F1 is the weighted harmonic mean of precision and recall. The closer the value of the F1 score is to 1.0, the better the expected performance of the model is.\n",
    "* $Support:$ Support is the number of actual occurrences of the class in the dataset. It doesn’t vary between models, it just diagnoses the performance evaluation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score\n",
    "Model accuracy is a machine learning model performance metric that is defined as the ratio of true positives and true negatives to all positive and negative observations. In other words, accuracy tells us how often we can expect our machine learning model will correctly predict an outcome out of the total number of times it made predictions\n",
    "<br> Closer the Accuracy score to 1 ,better is the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 2]\n",
      " [3 4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.62      0.71      0.67         7\n",
      "           Y       0.67      0.57      0.62         7\n",
      "\n",
      "    accuracy                           0.64        14\n",
      "   macro avg       0.65      0.64      0.64        14\n",
      "weighted avg       0.65      0.64      0.64        14\n",
      "\n",
      "Accuracy Score = 0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "# Importing Numpy\n",
    "import numpy as np\n",
    "# Importing confusion_matrix, classification_report, accuracy_score from sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Defining Predicted array\n",
    "predicted = np.array([\"Y\",\"Y\",\"N\",\"N\",\"Y\",\"N\",\"Y\",\"Y\",\"N\",\"N\",\"N\",\"N\",\"Y\",\"N\"],dtype=object)\n",
    "\n",
    "# Defining Existing array\n",
    "existing = np.array([\"Y\",\"N\",\"N\",\"N\",\"Y\",\"N\",\"Y\",\"N\",\"Y\",\"Y\",\"Y\",\"N\",\"Y\",\"N\"],dtype=object)\n",
    "\n",
    "# Showing the confusion_matrix\n",
    "print(confusion_matrix(existing, predicted))\n",
    "# Showing the classification_report\n",
    "print(classification_report(existing, predicted))\n",
    "# Showing the accuracy_score\n",
    "print(\"Accuracy Score =\", accuracy_score(existing,predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8f38a65f95ae979fba470ab93511c61f11d2b36188dbafc11a6a501c4e93194"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
