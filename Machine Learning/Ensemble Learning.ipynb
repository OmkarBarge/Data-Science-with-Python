{"cells":[{"cell_type":"markdown","metadata":{"id":"5FF1KmPFpaiu"},"source":["### Ensemble Learning\n","1) They combine the decisions from multiple models to improve the overall performance. <br>\n","2) This approach allows the production of better predictive performance compared to a single model. Basic idea is to learn a set of classifiers (experts) and to allow them to vote.<br>\n","3) The different ensemble learning methods are:<br>\n","\n","#### a) Bagging (Bootstrap Aggregation)\n","\n","Bootstrap - Selection with replacement<br>\n","Aggregation - Averaging<br>\n","\n","1) It is parallel processing<br>\n","2) Bootstrap Aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression.<br>\n","3) It decreases the variance and helps to avoid overfitting. It is usually applied to decision tree methods. Bagging is a special case of the model averaging approach.<br> \n","4) All the trees used in Bagging apparoach have equal weightage<br>\n","\n","\n","Examples of Bagging Algroithms - RandomForest\n","\n","#### b) Boosting\n","1) Boosting in based on sequential processing. <br>\n","2) Boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers. \n","3) It is done by building a model by using weak models in series. Firstly, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models is added.<br>\n","4) All the trees, used in Boosting dont have equal weightage<br> \n","\n","Examples of Boosting Algroithms - AdaBoost, XGBoost\n"],"id":"5FF1KmPFpaiu"},{"cell_type":"markdown","metadata":{"id":"lVgnGRPepai2"},"source":["### AdaBoost (Adaptive Boosting)\n","\n","### Working of AdaBoost Classifier\n","\n","1) Initialise the dataset and assign equal weight to each of the data point.<br>\n","For N data points, the weights assigned to each data point is 1/N<br>\n","2) Decision stump is used(1 root node and 2 leaf nodes).Howevere it can be changed using the max depth parameter<br>\n","3) Provide this as input to the model(Decision Stump) and identify the wrongly classified data points.<br>\n","4) Increase the weight of the wrongly classified data points and decrease the weights of correctly classified data points. And then normalize the weights of all data points.<br>\n","5) For the generation of second Decision Stump,the chances are high that the model selects those data points which are incorrectly classified by the first Decision Stump and the data points correctly classified have compartively low chcnces to be a part of Second Decision Stump<br>\n","6) Repeat the step 3 until desired results are not obtained.\n"],"id":"lVgnGRPepai2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MKsphwypai3"},"outputs":[],"source":[],"id":"5MKsphwypai3"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}